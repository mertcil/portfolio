<!DOCTYPE html><!--7Eya7rACpqsxVH__5tsT8--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/cfe20472333721eb.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/467c7c70b53e8d19.js"/><script src="/_next/static/chunks/7108e421821278fa.js" async=""></script><script src="/_next/static/chunks/335c0ab066e717f3.js" async=""></script><script src="/_next/static/chunks/3c70d6631ca3b816.js" async=""></script><script src="/_next/static/chunks/turbopack-995b50531fc12fb0.js" async=""></script><script src="/_next/static/chunks/e851924282456e89.js" async=""></script><script src="/_next/static/chunks/f0dec9012faf9135.js" async=""></script><script src="/_next/static/chunks/be0da13af251a44b.js" async=""></script><script src="/_next/static/chunks/397f9c9a727dd4de.js" async=""></script><script src="/_next/static/chunks/ebf79ac48e104c07.js" async=""></script><script src="/_next/static/chunks/59aa0cc92db52650.js" async=""></script><script src="/_next/static/chunks/ba374fad3111c2b3.js" async=""></script><script src="/_next/static/chunks/20fc55d672411081.js" async=""></script><script src="/_next/static/chunks/a616032c9b907623.js" async=""></script><title>Mevlut Mert CIL | Full Stack Developer</title><meta name="description" content="Portfolio of a full-stack developer"/><link rel="icon" href="/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><script src="/_next/static/chunks/467c7c70b53e8d19.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[63980,[\"/_next/static/chunks/e851924282456e89.js\",\"/_next/static/chunks/f0dec9012faf9135.js\",\"/_next/static/chunks/be0da13af251a44b.js\"],\"default\"]\n3:I[61689,[\"/_next/static/chunks/e851924282456e89.js\",\"/_next/static/chunks/f0dec9012faf9135.js\",\"/_next/static/chunks/be0da13af251a44b.js\"],\"default\"]\n4:I[44858,[\"/_next/static/chunks/e851924282456e89.js\",\"/_next/static/chunks/f0dec9012faf9135.js\",\"/_next/static/chunks/be0da13af251a44b.js\"],\"default\"]\n5:I[45469,[\"/_next/static/chunks/397f9c9a727dd4de.js\",\"/_next/static/chunks/ebf79ac48e104c07.js\"],\"default\"]\n6:I[61117,[\"/_next/static/chunks/e851924282456e89.js\",\"/_next/static/chunks/f0dec9012faf9135.js\",\"/_next/static/chunks/be0da13af251a44b.js\",\"/_next/static/chunks/59aa0cc92db52650.js\"],\"default\"]\n7:I[8027,[\"/_next/static/chunks/397f9c9a727dd4de.js\",\"/_next/static/chunks/ebf79ac48e104c07.js\"],\"default\"]\n8:I[28688,[\"/_next/static/chunks/e851924282456e89.js\",\"/_next/static/chunks/f0dec9012faf9135.js\",\"/_next/static/chunks/be0da13af251a44b.js\"],\"default\"]\n9:I[89135,[\"/_next/static/chunks/e851924282456e89.js\",\"/_next/static/chunks/f0dec9012faf9135.js\",\"/_next/static/chunks/be0da13af251a44b.js\",\"/_next/static/chunks/ba374fad3111c2b3.js\"],\"default\"]\na:I[16760,[\"/_next/static/chunks/e851924282456e89.js\",\"/_next/static/chunks/f0dec9012faf9135.js\",\"/_next/static/chunks/be0da13af251a44b.js\",\"/_next/static/chunks/20fc55d672411081.js\"],\"default\"]\nc:I[15638,[\"/_next/static/chunks/397f9c9a727dd4de.js\",\"/_next/static/chunks/ebf79ac48e104c07.js\"],\"OutletBoundary\"]\nd:\"$Sreact.suspense\"\nf:I[15638,[\"/_next/static/chunks/397f9c9a727dd4de.js\",\"/_next/static/chunks/ebf79ac48e104c07.js\"],\"ViewportBoundary\"]\n11:I[15638,[\"/_next/static/chunks/397f9c9a727dd4de.js\",\"/_next/static/chunks/ebf79ac48e104c07.js\"],\"MetadataBoundary\"]\n13:I[41505,[],\"default\"]\n:HL[\"/_next/static/chunks/cfe20472333721eb.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"7Eya7rACpqsxVH_-5tsT8\",\"c\":[\"\",\"posts\",\"2024-02-10-llms-in-development\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"2024-02-10-llms-in-development\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/cfe20472333721eb.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/e851924282456e89.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/f0dec9012faf9135.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/be0da13af251a44b.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"children\":[[\"$\",\"$L4\",null,{}],[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$6\",\"errorStyles\":[],\"errorScripts\":[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/59aa0cc92db52650.js\",\"async\":true}]],\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"$L8\",null,{}]]}]}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$9\",\"errorStyles\":[],\"errorScripts\":[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/ba374fad3111c2b3.js\",\"async\":true}]],\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$a\",\"errorStyles\":[],\"errorScripts\":[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/20fc55d672411081.js\",\"async\":true}]],\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$Lb\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/a616032c9b907623.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$Lc\",null,{\"children\":[\"$\",\"$d\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@e\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$Lf\",null,{\"children\":\"$@10\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L11\",null,{\"children\":[\"$\",\"$d\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@12\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$13\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"14:I[50588,[\"/_next/static/chunks/e851924282456e89.js\",\"/_next/static/chunks/f0dec9012faf9135.js\",\"/_next/static/chunks/be0da13af251a44b.js\",\"/_next/static/chunks/a616032c9b907623.js\"],\"default\"]\n15:T1daa,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eLarge Language Models in Software Development: Beyond Copilot\u003c/h1\u003e\n\u003cp\u003eLLMs have evolved beyond code generation. They're now used for documentation, testing, debugging, and architectural decisions.\u003c/p\u003e\n\u003cp\u003eTeams that treat LLMs as pair-programming partners see the biggest productivity gains. Keep a library of successful prompts, capture examples of high-quality responses, and routinely coach the model with context from your domain. These practices turn a generic assistant into a bespoke engineering teammate that understands your naming conventions, logging strategy, and acceptance criteria.\u003c/p\u003e\n\u003ch2\u003eCode Generation and Contextual Assistance\u003c/h2\u003e\n\u003cp\u003eLLMs excel at translating natural language specifications into working code. Whether scaffolding a new API endpoint, implementing a design pattern, or converting between languages, these models save hours of boilerplate work.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-javascript\"\u003e// Prompt: \"Create a user authentication middleware for Express\"\r\nconst authMiddleware = (req, res, next) =\u003e {\r\n  const token = req.headers.authorization?.split(' ')[1];\r\n  if (!token) return res.status(401).json({ error: 'Unauthorized' });\r\n\r\n  try {\r\n    const decoded = jwt.verify(token, process.env.JWT_SECRET);\r\n    req.user = decoded;\r\n    next();\r\n  } catch (error) {\r\n    res.status(403).json({ error: 'Invalid token' });\r\n  }\r\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe key to effective code generation is specificity. Vague prompts yield generic solutions; detailed prompts that include project conventions, error handling requirements, and performance constraints produce production-ready code. Maintain a prompt library with templates for common tasks—database migrations, REST controllers, React components—and evolve them based on team feedback.\u003c/p\u003e\n\u003cp\u003eLLMs also assist with code comprehension. Paste a complex function and ask for an explanation, request refactoring suggestions, or identify potential bugs. This is invaluable when working with legacy codebases or unfamiliar languages, turning hours of documentation diving into minutes of interactive Q\u0026#x26;A.\u003c/p\u003e\n\u003cp\u003eIntegrate LLMs into your IDE with plugins like GitHub Copilot, Codeium, or Continue.dev. Context-aware suggestions that reference your current file, imported libraries, and project structure outperform generic completions. Over time, these tools learn from your acceptance patterns, surfacing increasingly relevant proposals.\u003c/p\u003e\n\u003ch2\u003eTest Generation and Quality Assurance\u003c/h2\u003e\n\u003cp\u003eWriting comprehensive test suites is time-consuming. LLMs accelerate this by generating test cases from function signatures, identifying edge cases, and creating mock data.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-typescript\"\u003e// LLM generates comprehensive tests from function\r\ndescribe('calculateOrderTotal', () =\u003e {\r\n  test('adds items correctly', () =\u003e {\r\n    const result = calculateOrderTotal([{price: 10}, {price: 20}]);\r\n    expect(result).toBe(30);\r\n  });\r\n\r\n  test('applies discount', () =\u003e {\r\n    const result = calculateOrderTotal([{price: 100}], { discount: 0.1 });\r\n    expect(result).toBe(90);\r\n  });\r\n\r\n  test('handles empty cart', () =\u003e {\r\n    expect(calculateOrderTotal([])).toBe(0);\r\n  });\r\n\r\n  test('throws error on negative prices', () =\u003e {\r\n    expect(() =\u003e calculateOrderTotal([{price: -10}])).toThrow();\r\n  });\r\n\r\n  test('handles floating point precision', () =\u003e {\r\n    const result = calculateOrderTotal([{price: 0.1}, {price: 0.2}]);\r\n    expect(result).toBeCloseTo(0.3);\r\n  });\r\n});\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBeyond unit tests, LLMs help with integration tests, property-based tests, and even visual regression tests by generating Playwright or Cypress scripts. Provide the model with acceptance criteria or user stories, and it drafts test scenarios that validate behavior from a product perspective.\u003c/p\u003e\n\u003cp\u003eLLMs can also review existing tests for coverage gaps. Feed your codebase and test suite into the model, then ask which branches, error paths, or boundary conditions lack assertions. This analysis complements traditional coverage tools by highlighting logical gaps rather than just line coverage.\u003c/p\u003e\n\u003cp\u003eQuality assurance extends to documentation. LLMs generate API reference docs from code comments, update READMEs when functionality changes, and draft migration guides for breaking changes. Automated documentation stays consistent and current, reducing onboarding friction for new contributors.\u003c/p\u003e\n\u003cp\u003eLLMs augment developer capabilities. They handle boilerplate, generate tests, suggest architectural improvements, and keep documentation synchronized. The future of development is human judgment + AI assistance, where engineers focus on strategic decisions while models handle repetitive mechanics.\u003c/p\u003e\n\u003ch2\u003eArchitecture Guidance and Knowledge Capture\u003c/h2\u003e\n\u003cp\u003eBeyond individual functions, LLMs excel at synthesizing architecture choices. Provide the model with ADRs, API contracts, and non-functional requirements, then ask for trade-off analyses between patterns like hexagonal architecture vs. layered services. The assistant can flag gaps, point to prior decisions, or even draft updates to documentation that keep the system of record current.\u003c/p\u003e\n\u003cp\u003eAnother high-leverage pattern is turning ad-hoc conversations into durable knowledge. After incident reviews or spike solutions, feed the transcript to an LLM and request a concise summary with action items, risks, and owners. Publish the output in an internal handbook so new teammates ramp up with curated context instead of trawling through chat histories.\u003c/p\u003e\n\u003ch2\u003eGovernance, Security, and Cost Controls\u003c/h2\u003e\n\u003cp\u003eAdoption demands clear guardrails. Define policies for handling sensitive data—mask credentials before sharing with a hosted LLM and prefer private deployments for regulated workloads. Track token usage, set spending limits, and monitor accuracy with periodic benchmarking against ground-truth answers. Establish a feedback loop where engineers rate responses so the platform team can tune prompts, swap models, or add validation steps when hallucinations appear.\u003c/p\u003e\n\u003cp\u003eThe best LLM programs blend automation with review. Let the assistant draft pull requests or refactor plans, but require engineers to validate logic and run tests. That combination preserves human accountability while extracting real leverage from frontier models.\u003c/p\u003e\n\u003ch2\u003eMeasuring Impact and ROI\u003c/h2\u003e\n\u003cp\u003eAdoption should be data-driven. Track metrics such as:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSuggestion acceptance rate per team\u003c/li\u003e\n\u003cli\u003eTime-to-ship for features with and without LLM support\u003c/li\u003e\n\u003cli\u003eNumber of defects caught during AI-assisted reviews\u003c/li\u003e\n\u003cli\u003eToken spend by environment (staging, production, sandbox)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCreate dashboards that correlate usage with outcomes. If bug counts spike after heavy AI assistance, invest in better guardrails or more targeted training. Conversely, if onboarding time drops, celebrate and share the playbook company-wide.\u003c/p\u003e\n\u003ch2\u003eHuman Factors and Change Management\u003c/h2\u003e\n\u003cp\u003eLLM tooling changes workflows and expectations. Communicate early about what success looks like, how data privacy is protected, and how performance evaluations will incorporate AI collaboration. Offer opt-in pilot groups, gather qualitative feedback, and adapt the rollout based on real-world experience.\u003c/p\u003e\n\u003cp\u003eRespect individual preferences—some engineers prefer lightweight chat interfaces, others want IDE integrations. Provide training for both and ensure accessibility for neurodiverse teammates by offering multiple interaction modalities (speech, text, visual prompts). AI adoption thrives when humans feel supported, not coerced.\u003c/p\u003e"])</script><script>self.__next_f.push([1,"b:[\"$\",\"$L14\",null,{\"title\":\"Large Language Models in Software Development: Beyond Copilot\",\"date\":\"2024-02-10\",\"author\":\"Mevlut Mert CIL\",\"category\":\"AI \u0026 Machine Learning\",\"tags\":[\"llm\",\"ai\",\"development\",\"gpt\",\"claude\"],\"htmlContent\":\"$15\"}]\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"16:I[29057,[\"/_next/static/chunks/397f9c9a727dd4de.js\",\"/_next/static/chunks/ebf79ac48e104c07.js\"],\"IconMark\"]\n12:[[\"$\",\"title\",\"0\",{\"children\":\"Mevlut Mert CIL | Full Stack Developer\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Portfolio of a full-stack developer\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L16\",\"3\",{}]]\ne:null\n"])</script></body></html>